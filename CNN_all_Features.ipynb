{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch import optim\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "from sklearn.neural_network import MLPClassifier\r\n",
    "from sklearn.metrics import confusion_matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def to_list(df_dict, heads):\r\n",
    "    # {key: [], ...}\r\n",
    "    out = {}\r\n",
    "    for _key in list(heads):\r\n",
    "        out[_key] = [df_dict[_key][key] for key in df_dict[_key].keys()]\r\n",
    "    return out\r\n",
    "\r\n",
    "def encode_input(data_range, data):\r\n",
    "    return data_range.index(data)\r\n",
    "\r\n",
    "def preprocess_data(df_list: dict, need_encoded: list, normalize=True) -> np.ndarray:\r\n",
    "    output = []\r\n",
    "    data_length = len(df_list['id'])\r\n",
    "    data_ranges = {k : list(set(df_list[k])) for k in need_encoded}\r\n",
    "    for i in range(data_length):\r\n",
    "        item = []\r\n",
    "        for key in list(df_list.keys())[1:-1]:\r\n",
    "            if key in need_encoded:\r\n",
    "                item.append(encode_input(data_ranges[key], df_list[key][i]))\r\n",
    "            else:\r\n",
    "                item.append(df_list[key][i])\r\n",
    "        output.append(item)\r\n",
    "    inputs_arr = np.array(output)\r\n",
    "    targets_arr = np.array(df_list['stroke'])\r\n",
    "    if normalize:\r\n",
    "        _range = np.max(inputs_arr, axis=0) - np.min(inputs_arr, axis=0)\r\n",
    "        inputs_arr = (inputs_arr-np.min(inputs_arr, axis=0)) / _range\r\n",
    "    return inputs_arr, targets_arr\r\n",
    "\r\n",
    "def prepare_data(inputs, targets, seed=1001):\r\n",
    "    positive_mask = targets == 1\r\n",
    "    negative_mask = targets == 0\r\n",
    "    n_minimum = min(np.sum(positive_mask), np.sum(negative_mask))\r\n",
    "    positive_inputs = inputs[positive_mask][0:n_minimum, :]\r\n",
    "    positive_targets = targets[positive_mask][0:n_minimum]\r\n",
    "    negative_inputs = inputs[negative_mask][0:n_minimum, :]\r\n",
    "    negative_targets = targets[negative_mask][0:n_minimum]\r\n",
    "    inputs = np.concatenate([positive_inputs, negative_inputs]).tolist()\r\n",
    "    targets = np.concatenate([positive_targets, negative_targets]).tolist()\r\n",
    "    np.random.seed(seed)\r\n",
    "    np.random.shuffle(inputs)\r\n",
    "    np.random.seed(seed)\r\n",
    "    np.random.shuffle(targets)\r\n",
    "    return np.array(inputs), np.array(targets)\r\n",
    "\r\n",
    "def metrics(y_pred, y_true):\r\n",
    "    _confusion_matrix = confusion_matrix(y_pred, y_true)\r\n",
    "    tp = _confusion_matrix[0,0]\r\n",
    "    fn = _confusion_matrix[1,0]\r\n",
    "    fp = _confusion_matrix[0,1]\r\n",
    "    tn = _confusion_matrix[1,1]\r\n",
    "    # metrics\r\n",
    "    precision = tp/(tp+fp)\r\n",
    "    recall = tp/(tp+fn)\r\n",
    "    fscore = 2*tp/(2*tp + fp + fn)\r\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\r\n",
    "    miss_rate = fn/(tn+tp)\r\n",
    "    fall_out_rate = fp/(fp+tn)\r\n",
    "    # return \r\n",
    "    return [precision, recall, fscore, accuracy, miss_rate, fall_out_rate]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path = './data/train_2v.csv'\r\n",
    "df = pd.read_csv(path)\r\n",
    "df_clear = df.dropna(axis=0)\r\n",
    "df_dict = df_clear.to_dict()\r\n",
    "heads = list(df_dict.keys())\r\n",
    "print(heads)\r\n",
    "df_list = to_list(df_dict, heads)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gender_range = list(set(df_list['gender']))\r\n",
    "married_range = list(set(df_list['ever_married']))\r\n",
    "work_range = list(set(df_list['work_type']))\r\n",
    "residence_range = list(set(df_list['Residence_type']))\r\n",
    "smoking_range = list(set(df_list['smoking_status']))\r\n",
    "need_encoded = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\r\n",
    "print(gender_range)\r\n",
    "print(married_range)\r\n",
    "print(work_range)\r\n",
    "print(residence_range)\r\n",
    "print(smoking_range)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# distribution \r\n",
    "head = 'age'\r\n",
    "plt.hist(df_list[head])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# MLP \r\n",
    "n_repeat = 100\r\n",
    "_metrics = []\r\n",
    "\r\n",
    "for i in range(n_repeat):\r\n",
    "    # generate data\r\n",
    "    seed = random.randint(1, 1000)\r\n",
    "    inputs, targets = preprocess_data(df_list, need_encoded)\r\n",
    "    inputs, targets = prepare_data(inputs, targets, seed)\r\n",
    "    n_samples = inputs.shape[0]\r\n",
    "    tr_inputs = inputs[0:int(n_samples*0.7), :]\r\n",
    "    tr_targets = targets[0:int(n_samples*0.7)]\r\n",
    "    te_inputs = inputs[int(n_samples*0.7):, :]\r\n",
    "    te_targets = targets[int(n_samples*0.7):]\r\n",
    "    # train clf\r\n",
    "    clf = MLPClassifier(learning_rate_init=1e-2,\r\n",
    "        solver='sgd',activation='relu',max_iter=500,\r\n",
    "        alpha=5e-4, hidden_layer_sizes=(16, 32, 16),random_state=1)\r\n",
    "    clf.fit(tr_inputs, tr_targets)\r\n",
    "    # evaluate clf\r\n",
    "    te_pred = clf.predict(te_inputs)\r\n",
    "    run_metrics = metrics(te_pred, te_targets)\r\n",
    "    print(i+1, \"complete\", run_metrics)\r\n",
    "    _metrics.append(run_metrics)\r\n",
    "\r\n",
    "_metrics = np.array(_metrics)\r\n",
    "_metrics = np.mean(_metrics, axis=0)\r\n",
    "print(_metrics)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# CNN\r\n",
    "epochs = 100\r\n",
    "batchsize = 16\r\n",
    "\r\n",
    "inputs, targets = preprocess_data(df_list, need_encoded)\r\n",
    "inputs, targets = prepare_data(inputs, targets)\r\n",
    "n_samples = inputs.shape[0]\r\n",
    "tr_inputs = inputs[0:int(n_samples*0.7), :].reshape((-1, 1, 2, 5))\r\n",
    "tr_targets = targets[0:int(n_samples*0.7)].reshape((-1, 1))\r\n",
    "te_inputs = inputs[int(n_samples*0.7):, :].reshape((-1, 1, 2, 5))\r\n",
    "te_targets = targets[int(n_samples*0.7):].reshape((-1, 1))\r\n",
    "\r\n",
    "class StrokePred(Dataset):\r\n",
    "    def __init__(self, inputs, targets) -> None:\r\n",
    "        super().__init__()\r\n",
    "        self.inputs = inputs\r\n",
    "        self.targets = targets\r\n",
    "        \r\n",
    "    def __len__(self):\r\n",
    "        return self.inputs.shape[0]\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        _input = torch.from_numpy(self.inputs[index]).type(torch.float32)\r\n",
    "        _target = torch.from_numpy(self.targets[index]).type(torch.float32)\r\n",
    "        return _input, _target\r\n",
    "\r\n",
    "class StrokePredModel(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(StrokePredModel, self).__init__()\r\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\r\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=2)\r\n",
    "\r\n",
    "        self.linear1 = nn.Linear(32, 16)\r\n",
    "        self.linear2 = nn.Linear(16, 1)\r\n",
    "\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        out = F.relu(self.conv1(x))\r\n",
    "        out = F.relu(self.conv2(out))\r\n",
    "        out = out.view((-1, 32))\r\n",
    "        out = F.relu(self.linear1(out))\r\n",
    "        return F.sigmoid(self.linear2(out))\r\n",
    "\r\n",
    "train_set = StrokePred(tr_inputs, tr_targets)\r\n",
    "val_set = StrokePred(te_inputs, te_targets)\r\n",
    "train_loader = DataLoader(train_set, batch_size=batchsize)\r\n",
    "val_loader = DataLoader(train_set, batch_size=1)\r\n",
    "\r\n",
    "model = StrokePredModel()\r\n",
    "citeration = nn.BCELoss()\r\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\r\n",
    "\r\n",
    "for i in range(epochs):\r\n",
    "    model.train()\r\n",
    "    loss_ = 0\r\n",
    "    acc_ = 0\r\n",
    "    val_acc = 0\r\n",
    "    for j, (input_, target_) in enumerate(train_loader):\r\n",
    "        optimizer.zero_grad()\r\n",
    "        out = model(input_)\r\n",
    "        loss = citeration(out, target_)\r\n",
    "\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "        pred = out.detach().numpy()\r\n",
    "        pred_ = np.zeros_like(pred)\r\n",
    "        pred_[pred>0.5] = 1\r\n",
    "        pred_ = pred_.astype('float')\r\n",
    "        acc = np.sum(pred_ == target_.numpy()) / batchsize\r\n",
    "        loss_ += loss.item()\r\n",
    "        acc_ += acc\r\n",
    "\r\n",
    "    model.eval()\r\n",
    "    for j, (input_, target_) in enumerate(val_loader):\r\n",
    "        out = model(input_)\r\n",
    "        pred = out.detach().numpy()\r\n",
    "        pred_ = np.zeros_like(pred)\r\n",
    "        pred_[pred>0.5] = 1\r\n",
    "        pred_ = pred_.astype('float')\r\n",
    "        acc = np.sum(pred_ == target_.numpy())\r\n",
    "        val_acc += acc\r\n",
    "\r\n",
    "    print(\"epochs: {}, loss: {}, val_acc: {}\".format(\r\n",
    "        i+1,\r\n",
    "        loss_ / len(train_loader), \r\n",
    "        val_acc / len(val_loader)))\r\n",
    "\r\n",
    "model.eval()\r\n",
    "preds = []\r\n",
    "labels = []\r\n",
    "for j, (input_, target_) in enumerate(val_loader):\r\n",
    "    out = model(input_)\r\n",
    "    pred = out.detach().numpy()\r\n",
    "    pred_ = np.zeros_like(pred)\r\n",
    "    pred_[pred>0.5] = 1\r\n",
    "    pred_ = pred_.astype('float')\r\n",
    "    preds.append(pred_[0][0])\r\n",
    "    labels.append(target_.numpy()[0][0])\r\n",
    "\r\n",
    "metrics = metrics(preds, labels)\r\n",
    "print(metrics)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "f07d5c506dc792c1d17042ce6d63d3539913070c7203ee1d707a2b2ce1ee992d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}