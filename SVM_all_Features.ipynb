{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(df_dict, heads):\n",
    "    # {key: [], ...}\n",
    "    out = {}\n",
    "    for _key in list(heads):\n",
    "        out[_key] = [df_dict[_key][key] for key in df_dict[_key].keys()]\n",
    "    return out\n",
    "\n",
    "def encode_input(data_range, data):\n",
    "    return data_range.index(data)\n",
    "\n",
    "def preprocess_data(df_list: dict, need_encoded: list, normalize=True) -> np.ndarray:\n",
    "    output = []\n",
    "    data_length = len(df_list['id'])\n",
    "    data_ranges = {k : list(set(df_list[k])) for k in need_encoded}\n",
    "    for i in range(data_length):\n",
    "        item = []\n",
    "        for key in list(df_list.keys())[1:-1]:\n",
    "            if key in need_encoded:\n",
    "                item.append(encode_input(data_ranges[key], df_list[key][i]))\n",
    "            else:\n",
    "                item.append(df_list[key][i])\n",
    "        output.append(item)\n",
    "    inputs_arr = np.array(output)\n",
    "    targets_arr = np.array(df_list['stroke'])\n",
    "    if normalize:\n",
    "        _range = np.max(inputs_arr, axis=0) - np.min(inputs_arr, axis=0)\n",
    "        inputs_arr = (inputs_arr-np.min(inputs_arr, axis=0)) / _range\n",
    "    return inputs_arr, targets_arr\n",
    "\n",
    "def prepare_data(inputs, targets, seed=1001):\n",
    "    positive_mask = targets == 1\n",
    "    negative_mask = targets == 0\n",
    "    n_minimum = min(np.sum(positive_mask), np.sum(negative_mask))\n",
    "    positive_inputs = inputs[positive_mask][0:n_minimum, :]\n",
    "    positive_targets = targets[positive_mask][0:n_minimum]\n",
    "    negative_inputs = inputs[negative_mask][0:n_minimum, :]\n",
    "    negative_targets = targets[negative_mask][0:n_minimum]\n",
    "    inputs = np.concatenate([positive_inputs, negative_inputs]).tolist()\n",
    "    targets = np.concatenate([positive_targets, negative_targets]).tolist()\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(inputs)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(targets)\n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "def metrics(y_pred, y_true):\n",
    "    _confusion_matrix = confusion_matrix(y_pred, y_true)\n",
    "    tp = _confusion_matrix[0,0]\n",
    "    fn = _confusion_matrix[1,0]\n",
    "    fp = _confusion_matrix[0,1]\n",
    "    tn = _confusion_matrix[1,1]\n",
    "    # metrics\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    fscore = 2*tp/(2*tp + fp + fn)\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    miss_rate = fn/(tn+tp)\n",
    "    fall_out_rate = fp/(fp+tn)\n",
    "    # return \n",
    "    return [precision, recall, fscore, accuracy, miss_rate, fall_out_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './dataset/train_2v.csv'\n",
    "df = pd.read_csv(path)\n",
    "df_clear = df.dropna(axis=0)\n",
    "df_dict = df_clear.to_dict()\n",
    "heads = list(df_dict.keys())\n",
    "df_list = to_list(df_dict, heads)\n",
    "need_encoded = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "n_repeat = 100\n",
    "_metrics = []\n",
    "\n",
    "for i in range(n_repeat):\n",
    "    # generate data\n",
    "    seed = random.randint(1, 1000)\n",
    "    inputs, targets = preprocess_data(df_list, need_encoded)\n",
    "    inputs, targets = prepare_data(inputs, targets, seed)\n",
    "    n_samples = inputs.shape[0]\n",
    "    tr_inputs = inputs[0:int(n_samples*0.7), :]\n",
    "    tr_targets = targets[0:int(n_samples*0.7)]\n",
    "    te_inputs = inputs[int(n_samples*0.7):, :]\n",
    "    te_targets = targets[int(n_samples*0.7):]\n",
    "    # train clf\n",
    "    poly_kernel_svm_clf = Pipeline([ ( \"scaler\", StandardScaler()),\n",
    "                                 (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=5, C=15))\n",
    "                                ])\n",
    "    poly_kernel_svm_clf.fit(tr_inputs, tr_targets)\n",
    "    # evaluate clf\n",
    "    te_pred = poly_kernel_svm_clf.predict(te_inputs)\n",
    "    te_pred[te_pred>0.5] = 1\n",
    "    te_pred[te_pred<0.5] = 0\n",
    "    run_metrics = metrics(te_pred, te_targets)\n",
    "    print(i+1, \"complete\", run_metrics)\n",
    "    _metrics.append(run_metrics)\n",
    "\n",
    "run_metrics = np.array(_metrics)\n",
    "mean_metrics = np.mean(run_metrics, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\\n========= MEAN OF 100 Experiments ========\\n\")\n",
    "# 100 experiments => run_metrics\n",
    "for i, key in enumerate([\"precision\", \"recall\", \"fscore\", \"accuracy\", \"miss_rate\", \"fall_out_rate\"]): \n",
    "    print(key, mean_metrics[i])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f07d5c506dc792c1d17042ce6d63d3539913070c7203ee1d707a2b2ce1ee992d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
