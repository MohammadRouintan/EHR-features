{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.neural_network import MLPClassifier\r\n",
    "from sklearn.metrics import confusion_matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def to_list(df_dict, heads):\r\n",
    "    # {key: [], ...}\r\n",
    "    out = {}\r\n",
    "    for _key in list(heads):\r\n",
    "        out[_key] = [df_dict[_key][key] for key in df_dict[_key].keys()]\r\n",
    "    return out\r\n",
    "\r\n",
    "def encode_input(data_range, data):\r\n",
    "    return data_range.index(data)\r\n",
    "\r\n",
    "def preprocess_data(df_list: dict, need_encoded: list, normalize=True) -> np.ndarray:\r\n",
    "    output = []\r\n",
    "    data_length = len(df_list['id'])\r\n",
    "    data_ranges = {k : list(set(df_list[k])) for k in need_encoded}\r\n",
    "    for i in range(data_length):\r\n",
    "        item = []\r\n",
    "        for key in list(df_list.keys())[1:-1]:\r\n",
    "            if key in need_encoded:\r\n",
    "                item.append(encode_input(data_ranges[key], df_list[key][i]))\r\n",
    "            else:\r\n",
    "                item.append(df_list[key][i])\r\n",
    "        output.append(item)\r\n",
    "    inputs_arr = np.array(output)\r\n",
    "    targets_arr = np.array(df_list['stroke'])\r\n",
    "    if normalize:\r\n",
    "        _range = np.max(inputs_arr, axis=0) - np.min(inputs_arr, axis=0)\r\n",
    "        inputs_arr = (inputs_arr-np.min(inputs_arr, axis=0)) / _range\r\n",
    "    return inputs_arr, targets_arr\r\n",
    "\r\n",
    "def prepare_data(inputs, targets, seed=1001):\r\n",
    "    positive_mask = targets == 1\r\n",
    "    negative_mask = targets == 0\r\n",
    "    n_minimum = min(np.sum(positive_mask), np.sum(negative_mask))\r\n",
    "    positive_inputs = inputs[positive_mask][0:n_minimum, :]\r\n",
    "    positive_targets = targets[positive_mask][0:n_minimum]\r\n",
    "    negative_inputs = inputs[negative_mask][0:n_minimum, :]\r\n",
    "    negative_targets = targets[negative_mask][0:n_minimum]\r\n",
    "    inputs = np.concatenate([positive_inputs, negative_inputs]).tolist()\r\n",
    "    targets = np.concatenate([positive_targets, negative_targets]).tolist()\r\n",
    "    np.random.seed(seed)\r\n",
    "    np.random.shuffle(inputs)\r\n",
    "    np.random.seed(seed)\r\n",
    "    np.random.shuffle(targets)\r\n",
    "    return np.array(inputs), np.array(targets)\r\n",
    "\r\n",
    "def metrics(y_pred, y_true):\r\n",
    "    _confusion_matrix = confusion_matrix(y_pred, y_true)\r\n",
    "    tp = _confusion_matrix[0,0]\r\n",
    "    fn = _confusion_matrix[1,0]\r\n",
    "    fp = _confusion_matrix[0,1]\r\n",
    "    tn = _confusion_matrix[1,1]\r\n",
    "    # metrics\r\n",
    "    precision = tp/(tp+fp)\r\n",
    "    recall = tp/(tp+fn)\r\n",
    "    fscore = 2*tp/(2*tp + fp + fn)\r\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\r\n",
    "    miss_rate = fn/(tn+tp)\r\n",
    "    fall_out_rate = fp/(fp+tn)\r\n",
    "    # return \r\n",
    "    return [precision, recall, fscore, accuracy, miss_rate, fall_out_rate]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "path = './data/train_2v.csv'\r\n",
    "df = pd.read_csv(path)\r\n",
    "df_clear = df.dropna(axis=0)\r\n",
    "df_dict = df_clear.to_dict()\r\n",
    "heads = list(df_dict.keys())\r\n",
    "print(heads)\r\n",
    "df_list = to_list(df_dict, heads)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['id', 'gender', 'age', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'avg_glucose_level', 'bmi', 'smoking_status', 'stroke']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "gender_range = list(set(df_list['gender']))\r\n",
    "married_range = list(set(df_list['ever_married']))\r\n",
    "work_range = list(set(df_list['work_type']))\r\n",
    "residence_range = list(set(df_list['Residence_type']))\r\n",
    "smoking_range = list(set(df_list['smoking_status']))\r\n",
    "need_encoded = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\r\n",
    "print(gender_range)\r\n",
    "print(married_range)\r\n",
    "print(work_range)\r\n",
    "print(residence_range)\r\n",
    "print(smoking_range)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Male', 'Other', 'Female']\n",
      "['No', 'Yes']\n",
      "['Govt_job', 'Self-employed', 'Never_worked', 'Private', 'children']\n",
      "['Rural', 'Urban']\n",
      "['formerly smoked', 'never smoked', 'smokes']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# distribution \r\n",
    "head = 'age'\r\n",
    "plt.hist(df_list[head])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inputs, targets = preprocess_data(df_list, need_encoded)\r\n",
    "inputs, targets = prepare_data(inputs, targets)\r\n",
    "n_samples = inputs.shape[0]\r\n",
    "tr_inputs = inputs[0:int(n_samples*0.7), :]\r\n",
    "tr_targets = targets[0:int(n_samples*0.7)]\r\n",
    "te_inputs = inputs[int(n_samples*0.7):, :]\r\n",
    "te_targets = targets[int(n_samples*0.7):]\r\n",
    "\r\n",
    "clf = MLPClassifier(learning_rate_init=1e-2,\r\n",
    "    solver='sgd',activation='relu',max_iter=500,\r\n",
    "    alpha=5e-4, hidden_layer_sizes=(32, 64, 32),random_state=1,\r\n",
    "    verbose=True)\r\n",
    "clf.fit(tr_inputs, tr_targets)\r\n",
    "# clf.score(tr_inputs, tr_targets)\r\n",
    "clf.score(te_inputs, te_targets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "n_repeat = 100\r\n",
    "_metrics = []\r\n",
    "\r\n",
    "for i in range(n_repeat):\r\n",
    "    # generate data\r\n",
    "    seed = random.randint(1, 1000)\r\n",
    "    inputs, targets = preprocess_data(df_list, need_encoded)\r\n",
    "    inputs, targets = prepare_data(inputs, targets, seed)\r\n",
    "    n_samples = inputs.shape[0]\r\n",
    "    tr_inputs = inputs[0:int(n_samples*0.7), :]\r\n",
    "    tr_targets = targets[0:int(n_samples*0.7)]\r\n",
    "    te_inputs = inputs[int(n_samples*0.7):, :]\r\n",
    "    te_targets = targets[int(n_samples*0.7):]\r\n",
    "    # train clf\r\n",
    "    clf = MLPClassifier(learning_rate_init=1e-2,\r\n",
    "        solver='sgd',activation='relu',max_iter=500,\r\n",
    "        alpha=5e-4, hidden_layer_sizes=(16, 32, 16),random_state=1)\r\n",
    "    clf.fit(tr_inputs, tr_targets)\r\n",
    "    # evaluate clf\r\n",
    "    te_pred = clf.predict(te_inputs)\r\n",
    "    run_metrics = metrics(te_pred, te_targets)\r\n",
    "    print(i+1, \"complete\", run_metrics)\r\n",
    "    _metrics.append(run_metrics)\r\n",
    "\r\n",
    "_metrics = np.array(_metrics)\r\n",
    "_metrics = np.mean(_metrics, axis=0)\r\n",
    "print(_metrics)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Software\\Anaconda3\\envs\\paddle\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 complete [0.7701863354037267, 0.7045454545454546, 0.7359050445103857, 0.729483282674772, 0.21666666666666667, 0.24183006535947713]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Software\\Anaconda3\\envs\\paddle\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 complete [0.75, 0.7222222222222222, 0.7358490566037735, 0.7446808510638298, 0.1836734693877551, 0.23353293413173654]\n",
      "3 complete [0.8037974683544303, 0.7055555555555556, 0.7514792899408284, 0.7446808510638298, 0.2163265306122449, 0.2080536912751678]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Software\\Anaconda3\\envs\\paddle\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4 complete [0.7592592592592593, 0.75, 0.754601226993865, 0.756838905775076, 0.1646586345381526, 0.23636363636363636]\n",
      "5 complete [0.75, 0.7354838709677419, 0.742671009771987, 0.7598784194528876, 0.164, 0.21839080459770116]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Software\\Anaconda3\\envs\\paddle\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:619: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6 complete [0.7960526315789473, 0.7515527950310559, 0.7731629392971247, 0.78419452887538, 0.15503875968992248, 0.18452380952380953]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Software\\Anaconda3\\envs\\paddle\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7 complete [0.7469879518072289, 0.7337278106508875, 0.7402985074626866, 0.7355623100303952, 0.1859504132231405, 0.2625]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Software\\Anaconda3\\envs\\paddle\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8 complete [0.8120805369127517, 0.7469135802469136, 0.7781350482315113, 0.790273556231003, 0.1576923076923077, 0.16766467065868262]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Software\\Anaconda3\\envs\\paddle\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9 complete [0.7816901408450704, 0.69375, 0.7350993377483444, 0.756838905775076, 0.19678714859437751, 0.1834319526627219]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Software\\Anaconda3\\envs\\paddle\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10 complete [0.8137931034482758, 0.7151515151515152, 0.7612903225806451, 0.7750759878419453, 0.1843137254901961, 0.16463414634146342]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Software\\Anaconda3\\envs\\paddle\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11 complete [0.7712418300653595, 0.7329192546583851, 0.7515923566878981, 0.7629179331306991, 0.17131474103585656, 0.20833333333333334]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Software\\Anaconda3\\envs\\paddle\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12 complete [0.7529411764705882, 0.7852760736196319, 0.7687687687687688, 0.7659574468085106, 0.1388888888888889, 0.25301204819277107]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('paddle': conda)"
  },
  "interpreter": {
   "hash": "bb121cf8bb4c457d868b9ff95e13257f4f865030a3f18e6d54bf297e562640e4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}